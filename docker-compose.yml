x-airflow-common: &airflow-common
  build:
    context: ./images
    dockerfile: airflow.Dockerfile
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_CORE_EXECUTOR}
    AIRFLOW__CORE__AUTH_MANAGER: ${AIRFLOW_CORE_AUTH_MANAGER}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${DB_CONTAINER_NAME}/${DB_NAME}
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://${AIRFLOW_API_CONTAINER_NAME}:${AIRFLOW_API_PORT}/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET:-airflow_jwt_secret}
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/scripts
    - mlflow-data:${MLFLOW_DATA_PATH}
  user: "${AIRFLOW_UID}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
  postgres:
    image: ${DB_IMAGE}
    container_name: ${DB_CONTAINER_NAME}
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    volumes:
      - postgres-data:${DB_DATA_PATH}
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "${DB_USER}" ]
      interval: 5s
      retries: 5
      start_period: 5s
    restart: always

  airflow-init:
    <<: *airflow-common
    container_name: ${AIRFLOW_INIT_CONTAINER_NAME}
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Creating missing opt dirs if missing:"
        echo
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow version:"
        /entrypoint airflow version
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Running airflow config list to create default config file if missing."
        echo
        /entrypoint airflow config list >/dev/null
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
        echo
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        echo
        echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
        echo
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_WWW_USER_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_WWW_USER_PASSWORD}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"

  airflow-apiserver:
    <<: *airflow-common
    container_name: ${AIRFLOW_API_CONTAINER_NAME}
    command: api-server
    ports:
      - "${AIRFLOW_API_PORT}:${AIRFLOW_API_PORT}"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:${AIRFLOW_API_PORT}/api/v2/version" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    container_name: ${AIRFLOW_DAG_PROCESSOR_CONTAINER_NAME}
    command: dag-processor
    healthcheck:
      test: [ "CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"' ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: ${AIRFLOW_SCHEDULER_CONTAINER_NAME}
    command: scheduler
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8974/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  mlflow:
    build:
      context: ./images
      dockerfile: mlflow.Dockerfile
    container_name: ${MLFLOW_CONTAINER_NAME}
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:${MLFLOW_PORT}/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - mlflow-data:${MLFLOW_DATA_PATH}
    restart: always

volumes:
  postgres-data:
  mlflow-data:
