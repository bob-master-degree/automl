x-airflow-common: &airflow-common
  build:
    context: ./docker/airflow
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_CORE_EXECUTOR}
    AIRFLOW__CORE__AUTH_MANAGER: ${AIRFLOW_CORE_AUTH_MANAGER}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${DB_CONTAINER}/${DB_NAME}
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://${AIRFLOW_API_CONTAINER}:${AIRFLOW_API_PORT}/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET:-airflow_jwt_secret}
    AIRFLOW__METRICS__STATSD_ON: 'True'
    AIRFLOW__METRICS__STATSD_HOST: ${STATSD_CONTAINER}
    AIRFLOW__METRICS__STATSD_PORT: ${STATSD_LISTEN_PORT}
    AIRFLOW__METRICS__STATSD_PREFIX: 'airflow'
    MLFLOW_TRACKING_URI: 'http://${MLFLOW_CONTAINER}:${MLFLOW_API_PORT}'
  env_file:
    - .kaggle.env
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/scripts:/opt/airflow/scripts
    - mlflow-data:${MLFLOW_DATA_PATH}
  user: "${AIRFLOW_UID}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
  postgres:
    image: ${DB_IMAGE}
    container_name: ${DB_CONTAINER}
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    volumes:
      - postgres-data:${DB_DATA_PATH}
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "${DB_USER}" ]
      interval: 5s
      retries: 5
      start_period: 5s
    restart: always

  airflow-init:
    <<: *airflow-common
    container_name: ${AIRFLOW_INIT_CONTAINER}
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Creating missing opt dirs if missing:"
        echo
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow version:"
        /entrypoint airflow version
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Running airflow config list to create default config file if missing."
        echo
        /entrypoint airflow config list >/dev/null
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
        echo
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        echo
        echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
        echo
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"

  airflow-apiserver:
    <<: *airflow-common
    container_name: ${AIRFLOW_API_CONTAINER}
    command: api-server
    ports:
      - "${AIRFLOW_API_PORT}:8080"
    healthcheck:
      test: [
        "CMD",
        "curl",
        "--fail",
        "http://localhost:8080/api/v2/version"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    container_name: ${AIRFLOW_DAG_PROCESSOR_CONTAINER}
    command: dag-processor
    healthcheck:
      test: [
        "CMD",
        'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"'
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: ${AIRFLOW_SCHEDULER_CONTAINER}
    command: scheduler
    healthcheck:
      test: [
        "CMD",
        "curl",
        "--fail",
        "http://localhost:8974/health"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  mlflow:
    build:
      context: docker/mlflow
      args:
        MLFLOW_DATA_PATH: ${MLFLOW_DATA_PATH}
    container_name: ${MLFLOW_CONTAINER}
    ports:
      - "${MLFLOW_API_PORT}:5000"
    healthcheck:
      test: [
        "CMD",
        "curl",
        "-f",
        "http://localhost:5000/health"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - mlflow-data:${MLFLOW_DATA_PATH}
    restart: always

  statsd-exporter:
    image: ${STATSD_IMAGE}
    container_name: ${STATSD_CONTAINER}
    entrypoint: statsd_exporter
    command:
      - "--statsd.listen-udp=:${STATSD_LISTEN_PORT}"
      - "--web.listen-address=:${STATSD_API_PORT}"
      - "--log.level=debug"
      - "--statsd.mapping-config=/etc/statsd-exporter/mapping.yml"
    ports:
      - "${STATSD_API_PORT}:${STATSD_API_PORT}"
      - "${STATSD_LISTEN_PORT}:${STATSD_LISTEN_PORT}"
    expose:
      - "${STATSD_LISTEN_PORT}/udp"
    volumes:
      - ${STATSD_CONFIG}:/etc/statsd-exporter/mapping.yml:ro
    restart: always

  prometheus:
    image: ${PROMETHEUS_IMAGE}
    container_name: ${PROMETHEUS_CONTAINER}
    command:
      - "--web.listen-address=:${PROMETHEUS_API_PORT}"
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    ports:
      - "${PROMETHEUS_API_PORT}:${PROMETHEUS_API_PORT}"
    volumes:
      - prometheus-data:${PROMETHEUS_DATA_PATH}
      - ${PROMETHEUS_CONFIG}:/etc/prometheus/prometheus.yml:ro
    restart: always

  grafana:
    image: ${GRAFANA_IMAGE}
    container_name: ${GRAFANA_CONTAINER}
    ports:
      - "${GRAFANA_API_PORT}:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
    volumes:
      - grafana-data:${GRAFANA_DATA_PATH}
      - ${GRAFANA_DATASOURCES}:/etc/grafana/provisioning/datasources/
    depends_on:
      - prometheus
    restart: always

volumes:
  postgres-data:
  mlflow-data:
  prometheus-data:
  grafana-data:
