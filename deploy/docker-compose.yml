x-airflow-common: &airflow-common
  build:
    context: ./docker/airflow
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_CORE_EXECUTOR}
    AIRFLOW__CORE__AUTH_MANAGER: ${AIRFLOW_CORE_AUTH_MANAGER}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${DB_CONTAINER_NAME}/${DB_NAME}
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://${AIRFLOW_API_CONTAINER_NAME}:${AIRFLOW_API_PORT}/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET:-airflow_jwt_secret}
    AIRFLOW__METRICS__STATSD_ON: 'True'
    AIRFLOW__METRICS__STATSD_HOST: "statsd-exporter"
    AIRFLOW__METRICS__STATSD_PORT: ${STATSD_PORT}
    AIRFLOW__METRICS__STATSD_PREFIX: 'airflow'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/scripts:/opt/airflow/scripts
    - mlflow-data:${MLFLOW_DATA_PATH}
  user: "${AIRFLOW_UID}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
  postgres:
    image: ${DB_IMAGE}
    container_name: ${DB_CONTAINER_NAME}
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    volumes:
      - postgres-data:${DB_DATA_PATH}
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "${DB_USER}" ]
      interval: 5s
      retries: 5
      start_period: 5s
    restart: always

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Creating missing opt dirs if missing:"
        echo
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow version:"
        /entrypoint airflow version
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Running airflow config list to create default config file if missing."
        echo
        /entrypoint airflow config list >/dev/null
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
        echo
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        echo
        echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
        echo
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_WWW_USER_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_WWW_USER_PASSWORD}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"

  airflow-apiserver:
    <<: *airflow-common
    container_name: ${AIRFLOW_API_CONTAINER_NAME}
    command: api-server
    ports:
      - "${AIRFLOW_API_PORT}:${AIRFLOW_API_PORT}"
    healthcheck:
      test: [
        "CMD",
        "curl",
        "--fail",
        "http://localhost:${AIRFLOW_API_PORT}/api/v2/version"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test: [
        "CMD",
        'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"'
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: [
        "CMD",
        "curl",
        "--fail",
        "http://localhost:${AIRFLOW_SCHEDULER_PORT}/health"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  mlflow:
    build:
      context: docker/mlflow
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    healthcheck:
      test: [
        "CMD",
        "curl",
        "-f",
        "http://localhost:${MLFLOW_PORT}/health"
      ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - mlflow-data:${MLFLOW_DATA_PATH}
    restart: always

  statsd-exporter:
    image: ${STATSD_IMAGE}
    volumes:
      - ${STATSD_LOCAL_CONFIG}:${STATSD_INTERNAL_CONFIG}:ro
    entrypoint: statsd_exporter
    command:
      - "--statsd.listen-udp=:${STATSD_PORT}"
      - "--web.listen-address=:${STATSD_WEB_PORT}"
      - "--log.level=debug"
      - "--statsd.mapping-config=${STATSD_INTERNAL_CONFIG}"
    ports:
      - "${STATSD_WEB_PORT}:${STATSD_WEB_PORT}"
      - "${STATSD_PORT}:${STATSD_PORT}"
    expose:
      - "${STATSD_PORT}/udp"
    restart: always

  prometheus:
    image: ${PROMETHEUS_IMAGE}
    command:
      - '--config.file=${PROMETHEUS_INTERNAL_CONFIG}'
      - '--storage.tsdb.path=/prometheus'
    ports:
      - "${PROMETHEUS_PORT}:${PROMETHEUS_PORT}"
    volumes:
      - prometheus-data:${PROMETHEUS_DATA_PATH}
      - ${PROMETHEUS_LOCAL_CONFIG}:${PROMETHEUS_INTERNAL_CONFIG}:ro
    restart: always

  grafana:
    image: ${GRAFANA_IMAGE}
    ports:
      - "${GRAFANA_PORT}:${GRAFANA_PORT}"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
    volumes:
      - grafana-data:${GRAFANA_DATA_PATH}
      - ${GRAFANA_LOCAL_DATASOURCES}:${GRAFANA_INTERNAL_DATASOURCES}
    depends_on:
      - prometheus
    restart: always

volumes:
  postgres-data:
  mlflow-data:
  prometheus-data:
  grafana-data:
